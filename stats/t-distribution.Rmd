---
title: "T Distribution"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```

## The t-distribution

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse) 
library(dslabs) 
data("polls_us_election_2016") 
ds_theme_set() 
polls <- polls_us_election_2016 |>  
  filter(state == "U.S." & enddate >= "2016-10-31" & 
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) |>  
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) 
one_poll_per_pollster <- polls |> group_by(pollster) |>  
  filter(enddate == max(enddate)) |> 
  ungroup() 
```

-   Above we made use of the CLT with a sample size of 15.

-   Because we are estimating a second parameters $\sigma$, further variability is introduced into our confidence interval which results in intervals that are too small.

## The t-distribution

-   For very large sample sizes this extra variability is negligible, but, in general, for values smaller than 30 we need to be cautious about using the CLT.

-   However, if the data in the urn is known to follow a normal distribution, then we actually have mathematical theory that tells us how much bigger we need to make the intervals to account for the estimation of $\sigma$.

-   Using this theory, we can construct confidence intervals for any $N$.

-   But again, this works only if **the data in the urn is known to follow a normal distribution**.

-   So for the 0, 1 data of our previous urn model, this theory definitely does not apply.

## The t-distribution

-   The statistic on which confidence intervals for $d$ are based is.

$$
Z = \frac{\bar{X} - d}{\sigma/\sqrt{N}} 
$$

-   CLT tells us that Z is approximately normally distributed with expected value 0 and standard error 1.

-   But in practice we don't know $\sigma$ so we use:

$$
t = \frac{\bar{X} - d}{s/\sqrt{N}} 
$$

-   This is referred to a *t-statistic*.

## The t-distribution

-   By substituting $\sigma$ with $s$ we introduce some variability.

-   The theory tells us that $t$ follows a *student t-distribution* with $N-1$ *degrees of freedom*.

-   The degrees of freedom is a parameter that controls the variability via fatter tails:

## The t-distribution

```{r t-distribution-examples, echo=FALSE}
x <- seq(-5,5, len=100) 
data.frame(x=x, Normal = dnorm(x, 0, 1), t_03 = dt(x,3), t_05 = dt(x,5), t_15=dt(x,15)) |> gather(distribution, f, -x) |> ggplot(aes(x,f, color = distribution)) + geom_line() +ylab("f(x)") 
```

## The t-distribution

-   If we are willing to assume the pollster effect data is normally distributed, based on the sample data $X_1, \dots, X_N$,

```{r poll-spread-qq-1, eval=FALSE}
one_poll_per_pollster |> 
  ggplot(aes(sample=spread)) + stat_qq() 
```

## The t-distribution

```{r poll-spread-qq-2, echo=FALSE}
one_poll_per_pollster |> 
  ggplot(aes(sample=spread)) + stat_qq() 
```

## The t-distribution

-   then $t$ follows a t-distribution with $N-1$ degrees of freedom.

-   So perhaps a better confidence interval for $d$ is:

```{r}
z <- qt(0.975,  nrow(one_poll_per_pollster)-1) 
one_poll_per_pollster |>  
  summarize(avg = mean(spread), 
            moe = z*sd(spread)/sqrt(length(spread))) |>  
  mutate(start = avg - moe, end = avg + moe)  
```


## The t-distribution

-   A bit larger than the one using normal is.

```{r}
qt(0.975, 14) 
```

-   is bigger than.

```{r}
qnorm(0.975) 
```

-   Using the t-distribution and the t-statistic is the basis for *t-tests*, widely used approach for computing p-values.

## The t-distribution

-   To learn more about t-tests, you can consult any statistics textbook.

-   The t-distribution can also be used to model errors in bigger deviations that are more likely than with the normal distribution, as seen in the densities we previously saw.

-   Fivethirtyeight uses the t-distribution to generate errors that better model the deviations we see in election data.

-   For example, in Wisconsin the average of six polls was 7% in favor of Clinton with a standard deviation of 1%, but Trump won by 0.7%.

-   Even after taking into account the overall bias, this 7.7% residual is more in line with t-distributed data than the normal distribution.

```{r}


## The t-distribution

data("polls_us_election_2016") 
polls_us_election_2016 |> 
  filter(state =="Wisconsin" & 
           enddate >="2016-10-31" &  
           (grade %in% c("A+","A","A-","B+") | is.na(grade))) |> 
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) |> 
  mutate(state = as.character(state)) |> 
  left_join(results_us_election_2016, by = "state") |> 
  mutate(actual = clinton/100 - trump/100) |> 
  summarize(actual = first(actual), avg = mean(spread),  
            sd = sd(spread), n = n()) |> 
  select(actual, avg, sd, n) 
```
