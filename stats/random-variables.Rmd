---
title: "Random Variables"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---
```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)
```



```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
dslabs::ds_theme_set()
set.seed(1)
```



## Sample model

Sample model for betting on red 1,000 times:

```{r}
n <- 1000
X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
```

## Define the total winnings 

```{r}
X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
S <- sum(X)
S
```


## Probability distribution 

Probability distribution for total winnings $S$: 

```{r}
n <- 1000
B <- 10000
roulette_winnings <- function(n){
  X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S <- replicate(B, roulette_winnings(n))
```

## Probability distribution

Now we can ask the following: in our simulations, how often did we get sums less than or equal to `a`?

```{r, eval=FALSE}
mean(S <= a)
```

This will be a very good approximation of $F(a)$ and we can easily answer the casino's question: how likely is it that we will lose money? We can see it is quite low:


```{r}
mean(S<0)
```

## Probability distribution 

We can visualize the distribution of $S$ by creating a histogram showing the probability $F(b)-F(a)$ for several intervals $(a,b]$: 

```{r normal-approximates-distribution, echo=FALSE}
s <- seq(min(S), max(S), length = 100)
normal_density <- data.frame(s = s, f=dnorm(s, mean(S), sd(S)))
data.frame(S=S) |> ggplot(aes(S, ..density..)) +
  geom_histogram(color = "black", binwidth = 10)  +
  ylab("Probability") + 
  geom_line(data = normal_density, mapping=aes(s,f), color="blue")
```


## Binomial distribution

* Statistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. 

* Specifically, in our example above, we can show that $(S+n)/2$ follows a binomial distribution. 

* We therefore do not need to run for Monte Carlo simulations to know the probability distribution of $S$. We did this for illustrative purposes. 


## Binomial distribution

We can use the function `dbinom` and `pbinom` to compute the probabilities exactly. For example, to compute $\mbox{Pr}(S < 0)$ we note that:

$$\mbox{Pr}(S < 0) = \mbox{Pr}((S+n)/2 < (0+n)/2)$$

and we can use the `pbinom` to compute $$\mbox{Pr}(S \leq 0)$$

```{r}
n <- 1000
pbinom(n/2, size = n, prob = 10/19)
```


## Binomial distribution

Because this is a discrete probability function, to get $\mbox{Pr}(S < 0)$ rather than $\mbox{Pr}(S \leq 0)$, we write:

```{r}
pbinom(n/2-1, size = n, prob = 10/19)
```

* For the details of the binomial distribution, you can consult any basic probability book or even [Wikipedia](https://en.wikipedia.org/w/index.php?title=Binomial_distribution]).


## Distributions versus probability distributions

* In the visualization section, we described how any list of numbers $x_1,\dots,x_n$ has a distribution.

* The definition is quite straightforward. We define $F(a)$ as the function that tells us what proportion of the list is less than or equal to $a$. 

## Distributions versus probability distributions

* Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. 

* These are defined with a straightforward operation of the vector containing the list of numbers `x`:

```{r, eval=FALSE}
m <- sum(x)/length(x)
s <- sqrt(sum((x - m)^2) / length(x))
```

## Distributions versus probability distributions

* A random variable $X$ has a distribution function. 

* To define this, we do not need a list of numbers. It is a theoretical concept. 

* In this case, we define the distribution as the $F(a)$ that answers the question: what is the probability that $X$ is less than or equal to $a$? There is no list of numbers. 

## Distributions versus probability distributions

* However, if $X$ is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. 

* In this case, the distribution of that list is the probability distribution of $X$ and the average and standard deviation of that list are the expected value and standard error of the random variable. 

* Another way to think about it that does not involve an urn is to run an infinite Monte Carlo simulation and use that list of numbers to define distribution. 

## Notation for random variables

* In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here.

* Lower case letters are used for observed values. You will see some notation that includes both. 

* For example, you will see events defined as $X \leq x$. 

* Here $X$ is a random variable, making it a random event, and $x$ is an arbitrary value and not random. 

## Example

* $X$  represent the number on a die roll

* $x$ will represent an actual value we see 1, 2, 3, 4, 5, or 6. 

* The probability of $X=x$ is 1/6 regardless of the observed value $x$. 

## Clarifying confusing language

* This notation is a bit strange because, when we ask questions about probability, $X$ is not an observed quantity. 

* Instead, it's a random quantity that we will see in the future. 

* We can talk about what we expect it to be, what values are probable, but not what it is. 

* But once we have data, we do see a realization of $X$. So data scientists talk of what could have been after we see what actually happened. 


## The expected value and standard error


* We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. 

* This will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.


## The expected value and standard error

* The first important concept to learn is the _expected value_. 
In statistics books, it is common to use letter $\mbox{E}$ like this:


$$\mbox{E}[X]$$ 

to denote the expected value of the random variable $X$.

##  Law of large numbers
 
* A random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.


## The expected value and standard error

* Theoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. 

* For example, __expected value of a random variable defined by one draw is the average of the numbers in the urn__. 

* In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:

$$
\mbox{E}[X] = (20 + -18)/38
$$

* which is about 5 cents. 

## Interpreting expected value

* It is a bit counter-intuitive to say that $X$ varies around 0.05, when the only values it takes is 1 and -1. 

* One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. 

* A Monte Carlo simulation confirms this:

```{r}
B <- 10^6
x <- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))
mean(x)
```

## Expected value for sample model

* In general, if the urn has two possible outcomes, say $a$ and $b$, with proportions $p$ and $1-p$ respectively, the average is: 

$$\mbox{E}[X] = ap + b(1-p)$$ 

* To see this, notice that if there are $n$ beads in the urn, then we have:
  - $np$ $a$s and 
  - $n(1-p)$ $b$s
  
* Because the average is the sum, 
  - $n\times a \times p + n\times b \times (1-p)$, 
  - divided by the total $n$, we get that the average is $ap + b(1-p)$.


## Expected value for sample model

* Now the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. 

* The first useful fact is that the *expected value of the sum of the draws* is:

$$
\mbox{}\mbox{number of draws } \times \mbox{ average of the numbers in the urn}
$$

## Standard Error

* So if 1,000 people play roulette, the casino expects to win, on average, about 1,000 $\times$ \$0.05  = \$50.  

* But this is an expected value. How different can one observation be from the expected value? 

* The casino really needs to know this: What is the range of possibilities? 

* If negative numbers are too likely, they will not install roulette wheels. 

## Standard Error

* Statistical theory once again answers this question.

* The _standard error_  (SE) gives us an idea of the size of the variation around the expected value.

* In statistics books, it's common to use: 

$$\mbox{SE}[X]$$ 

to denote the standard error of a random variable.

## Standard Error

* **If our draws are independent**, then the *standard error of the sum* is given by the equation:

$$
\sqrt{\mbox{number of draws }} \times \mbox{ SD of the numbers in the urn}
$$

* Using the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values $a$ and $b$ with proportions $p$ and $(1-p)$, respectively, the standard deviation is: 

$$\mid b - a \mid \sqrt{p(1-p)}.$$

## Standard Error

* So in our roulette example, the standard deviation of the values inside the urn is: $\mid 1 - (-1) \mid \sqrt{10/19 \times 9/19}$ or:


```{r}
2 * sqrt(90)/19
```

## Standard Error

* The standard error tells us the typical difference between a random variable and its expectation. 

* Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. 

* This makes sense since we either get 1 or -1, with 1 slightly favored over -1.

## Standard Error

Using the formula above, the sum of 1,000 people playing has standard error of about \$32:

```{r}
n <- 1000
sqrt(n) * 2 * sqrt(90)/19
```

* As a result, when 1,000 people bet on red, the casino is expected to win \$50 with a standard error of \$32. 

* It therefore seems like a safe bet. 

* But we still haven't answered the question: how likely is it to lose money? Here the CLT will help.


##  Technical note: population SD versus the sample SD


* The standard deviation of a list `x` (below we use heights as an example) is defined as the square root of the average of the squared differences:

```{r}
library(dslabs)
x <- heights$height
m <- mean(x)
s <- sqrt(mean((x-m)^2))
```

##  Technical note: population SD versus the sample SD

* Using mathematical notation we write:

$$
\mu = \frac{1}{n} \sum_{i=1}^n x_i \\
\sigma =  \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
$$

##  Technical note: population SD versus the sample SD

* However, be aware that the `sd` function returns a slightly different result:

```{r}
identical(s, sd(x))
s-sd(x)
```

##  Technical note: population SD versus the sample SD

* This is because the `sd` function R does not return the `sd` of the list, but rather uses a formula that estimates standard deviations of a population from a random sample $X_1, \dots, X_N$ which, for reasons not discussed here, divide the sum of squares by the $N-1$.


$$
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i, \,\,\,\,
s =  \sqrt{\frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2}
$$
##  Technical note: population SD versus the sample SD

* You can see that this is the case by typing:

```{r}
n <- length(x)
s-sd(x)*sqrt((n-1) / n)
```

##  Technical note: population SD versus the sample SD

For all the theory discussed here, you need to compute the actual standard deviation as defined:

```{r, eval = FALSE}
sqrt(mean((x-m)^2))
```

* So be careful when using the `sd` function in R.


## Central Limit Theorem

* The Central Limit Theorem (CLT) tells us that when the number of draws, also called the _sample size_, is large, the probability distribution of the sum of the independent draws is approximately normal.

* Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.

* Previously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. 

## Central Limit Theorem

* We also know that the same applies to probability distributions. 

* If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.

## Central Limit Theorem

* We previously ran this Monte Carlo simulation:

```{r}
n <- 1000
B <- 10000
roulette_winnings <- function(n){
  X <- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S <- replicate(B, roulette_winnings(n))
```

## Central Limit Theorem

* The Central Limit Theorem (CLT) tells us that the sum $S$ is approximated by a normal distribution. 

* Using the formulas above, we know that the expected value and standard error are:

```{r}
n * (20-18)/38 
sqrt(n) * 2 * sqrt(90)/19 
```

* The theoretical values above match those obtained with the Monte Carlo simulation:

```{r}
mean(S)
sd(S) # Here length(S) is so big we can use sd
```

## Central Limit Theorem

* Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:

```{r}
mu <- n * (20-18)/38
se <-  sqrt(n) * 2 * sqrt(90)/19 
pnorm(0, mu, se)
```

* which is also in very good agreement with our Monte Carlo result:

```{r}
mean(S < 0)
```


## How large is large in the CLT?

* The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. 

* In some specific instances, as few as 10 is enough. However, these should not be considered general rules. 

* Note, for example, that when the probability of success is very small, we need much larger sample sizes. 

## How large is large in the Central Limit Theorem?

* By way of illustration, let's consider the lottery. In the lottery, the chances of winning are less than 1 in a million. 

* Thousands of people play so the number of draws is very large. 

* Yet the number of winners, the sum of the draws, range between 0 and 4. 

* This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. 

* This is generally true when the probability of a success is very low.

* In these cases, the Poisson distribution is more appropriate. 

## How large is large in the Central Limit Theorem?

* You can examine the properties of the Poisson distribution using `dpois` and `ppois`.

* You can generate random variables following this distribution with `rpois`. 

* However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even [Wikipedia](https://en.wikipedia.org/w/index.php?title=Poisson_distribution)


## Statistical properties of averages

There are several useful mathematical results that we used above and often employ when working with data. We list them below.

1\. The expected value of the sum of random variables is the sum of each random variable's expected value. We can write it like this:

$$ 
\mbox{E}[X_1+X_2+\dots+X_n] =  \mbox{E}[X_1] + \mbox{E}[X_2]+\dots+\mbox{E}[X_n]
$$

If the $X$ are independent draws from the urn, then they all have the same expected value. Let's call it $\mu$ and thus:

$$ 
\mbox{E}[X_1+X_2+\dots+X_n]=  n\mu
$$

which is another way of writing the result we show above for the sum of draws.

## Statistical properties of averages

2\. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:

$$
\mbox{E}[aX] =  a\times\mbox{E}[X]
$$

## Statistical properties of averages

To see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it $\mu$ again:

$$
\begin{aligned}
\mbox{E}[(X_1+X_2+\dots+X_n) / n] &=   \mbox{E}[X_1+X_2+\dots+X_n] / n \\
&= n\mu/n \\
&= \mu 
\end{aligned}
$$

## Statistical properties of averages

3\. The square of the standard error of the sum of **independent** random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:

$$ 
\mbox{SE}[X_1+\dots+X_n] = \sqrt{\mbox{SE}[X_1]^2 +\dots+\mbox{SE}[X_n]^2  }
$$

## Statistical properties of averages

The square of the standard error is referred to as the _variance_ in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.
 
## Statistical properties of averages

4\. The standard error of a non-random constant times a random variable is the non-random constant times the random variable's standard error. As with the expectation:
$$
\mbox{SE}[aX] =  a \times \mbox{SE}[X]
$$

To see why this is intuitive, again think of units. 

## Statistical properties of averages

A consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of $n$ (the number of draws), call it $\sigma$:

$$
\begin{aligned}
\mbox{SE}[(X_1+\dots+X_n) / n] &=   \mbox{SE}[X_1+\dots+X_n]/n \\
&= \sqrt{\mbox{SE}[X_1]^2++\dots+\mbox{SE}[X_n]^2}/n \\
&= \sqrt{\sigma^2+\sigma^2+\dots+\sigma^2}/n\\
&= \sqrt{n\sigma^2}/n\\
&= \sigma / \sqrt{n}    
\end{aligned}
$$  
  
   
## Statistical properties of averages
 
5\. If $X$ is a normally distributed random variable, then if $a$ and $b$ are non-random constants, $aX + b$ is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by $a$, then shifting the center by $b$. 


## Notation

* Note that statistical textbooks use the Greek letters $\mu$ and $\sigma$ to denote the expected value and standard error, respectively. 

* This is because $\mu$ is the Greek letter for $m$, the first letter of _mean_, which is another term used for expected value. 

* Similarly, $\sigma$ is the Greek letter for $s$, the first letter of standard error. 

## Law of large numbers

* An important implication of the final result is that the standard error of the average becomes smaller and smaller as $n$ grows larger. 

* When $n$ is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. 

* This is known in statistical textbooks as the law of large numbers or the law of averages.


## Misinterpreting law of averages

* The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50\% heads and 50\% tails.

* A similar argument would be to say that red "is due" on the roulette wheel after seeing black come up five times in a row. 

* These events are independent so the chance of a coin landing heads is 50\% regardless of the previous 5. 

* This is also the case for the roulette outcome. 

## Misinterpreting law of averages

* The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50\% heads regardless of the outcome of the first five tosses. 

* Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.


