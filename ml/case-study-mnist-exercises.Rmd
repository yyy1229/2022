## Exercises 
1\. Use the `mnist_27` training set to build a model with several of the models available from the __caret__ package. For example, you can try these: 
```{r, eval = FALSE} 
models <- c("glm", "lda",  "naive_bayes",  "svmLinear", "gamboost",   
            "gamLoess", "qda", "knn", "kknn", "loclda", "gam", "rf",  
            "ranger","wsrf", "Rborist", "avNNet", "mlp", "monmlp", "gbm",  
            "adaboost", "svmRadial", "svmRadialCost", "svmRadialSigma") 
``` 
We have not explained many of these, but apply them anyway using `train` with all the default parameters. Keep the results in a list. You might need to install some packages. Keep in mind that you will likely get some warnings. 
2\. Now that you have all the trained models in a list, use `sapply` or `map` to create a matrix of predictions for the test set. You should end up with a matrix with `length(mnist_27$test$y)` rows and `length(models)` columns.  
3\. Now compute accuracy for each model on the test set. 
4\. Now build an ensemble prediction by majority vote and compute its accuracy. 
5\. Earlier we computed the accuracy of each method on the training set and noticed they varied.  Which individual methods do better than the ensemble?  
6\. It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object. 
7\. Now let's only consider the methods with an estimated accuracy of 0.8 when constructing the ensemble. What is the accuracy now? 
8\. __Advanced__: If two methods give results that are the same, ensembling them will not change the results at all. For each pair of metrics compare the percent of time they call the same thing. Then use the `heatmap` function to visualize the results. Hint: use the `method = "binary"` argument in the `dist` function. 
9\. __Advanced__: Note that each method can also produce an estimated conditional probability. Instead of majority vote we can take the average of these estimated conditional probabilities. For most methods, we can the use the `type = "prob"` in the train function. However, some of the methods require you to use the argument `trControl=trainControl(classProbs=TRUE)` when calling train. Also these methods do not work if classes have numbers as names. Hint: change the levels like this: 
```{r, eval = FALSE} 
dat$train$y <- recode_factor(dat$train$y, "2"="two", "7"="seven") 
dat$test$y <- recode_factor(dat$test$y, "2"="two", "7"="seven") 
``` 
10\. In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset. 

 
