## Exercises 
Generate a set of random predictors and outcomes like this: 
```{r, eval=FALSE} 
set.seed(1996) 
n <- 1000 
p <- 10000 
x <- matrix(rnorm(n * p), n, p) 
colnames(x) <- paste("x", 1:ncol(x), sep = "_") 
y <- rbinom(n, 1, 0.5) |> factor() 
x_subset <- x[ ,sample(p, 100)] 
``` 
1\. Because `x` and `y` are completely independent, you should not be able to predict `y` using `x` with accuracy larger than 0.5. Confirm this by running cross validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample `x_subset`. Use the subset when training the model. Hint: use the caret `train` function. The `results` component of the output of `train` shows you the accuracy. Ignore the warnings. 
2\. Now, instead of a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the $y=1$ group to those in the $y=0$ group, for each predictor, using a t-test. You can perform this step like this: 
```{r, eval=FALSE} 
devtools::install_bioc("genefilter") 
install.packages("genefilter") 
library(genefilter) 
tt <- colttests(x, y) 
``` 
Create a vector of the p-values and call it `pvals`. 
3\. Create an index `ind` with the column numbers of the predictors that were "statistically significantly" associated with `y`. Use a p-value cutoff of 0.01 to define "statistically significant". How many predictors survive this cutoff? 
4\. Re-run the cross validation but after redefining `x_subset` to be the subset of `x` defined by the columns showing "statistically significant" association with `y`. What is the accuracy now? 
5\. Re-run the cross validation again, but this time using kNN. Try out the following grid of tuning parameters: `k = seq(101, 301, 25)`. Make a plot of the resulting accuracy. 
6\. In exercises 3 and 4, we see that despite the fact that `x` and `y` are completely independent, we were able to predict `y` with accuracy higher than 70%. We must be doing something wrong then. What is it? 
a. The function `train` estimates accuracy on the same data it uses to train the algorithm. 
b. We are over-fitting the model by including 100 predictors. 
c. We used the entire dataset to select the columns used in the model. This step needs to be included as part of the algorithm. The cross validation was done **after** this selection. 
d. The high accuracy is just due to random variability. 
7\. Advanced. Re-do the cross validation but this time include the selection step in the cross validation. The accuracy should now be close to 50%. 
8\. Load the `tissue_gene_expression` dataset. Use the `train` function to predict tissue from gene expression. Use kNN. What `k` works best? 
```{r, include=FALSE} 
knitr::opts_chunk$set(out.width = "70%", out.extra = NULL) 
``` 
## Exercises 
1\. The `createResample` function can be used to create bootstrap samples. For example, we can create 10 bootstrap samples for the `mnist_27` dataset like this: 
```{r, eval=FALSE} 
set.seed(1995) 
indexes <- createResample(mnist_27$train$y, 10) 
``` 
How many times do `3`, `4`, and `7` appear in the first re-sampled index? 
2\. We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the re-sampled indexes. 
3\. Generate a random dataset like this: 
```{r, eval=FALSE} 
y <- rnorm(100, 0, 1) 
``` 
Estimate the 75th quantile, which we know is:  
```{r, eval = FALSE} 
qnorm(0.75) 
``` 
with the sample quantile: 
```{r, eval = FALSE} 
quantile(y, 0.75) 
``` 
Run a Monte Carlo simulation to learn the expected value and standard error of this random variable. 
4\. In practice, we can't run a Monte Carlo simulation because we don't know if `rnorm` is being used to simulate the data. Use the bootstrap to estimate the standard error using just the initial sample `y`. Use 10 bootstrap samples. 
5\. Redo exercise 4, but with 10,000 bootstrap samples. 

 
