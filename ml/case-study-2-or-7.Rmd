---
title: "Case Study 2 Or 7"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```

## Case study: is it a 2 or a 7?

- In the first simple example, we only had one predictor.

- We actually do not consider these machine learning challenges, which are characterized by cases with many predictors.

- Let's go back to the digits example in which we had 784 predictors.

- For illustrative purposes, we will start by simplifying this problem to one with two predictors and two classes.

- Specifically, we define the challenge as building an algorithm that can determine if a digit is a 2 or 7 from the predictors.



## Case study: is it a 2 or a 7?

- We are not quite ready to build algorithms with 784 predictors, so we will extract two simple predictors from the 784: the proportion of dark pixels that are in the upper left quadrant ($X_1$) and the lower right quadrant ($X_2$).

- We then select a random sample of 1,000 digits, 500 in the training set and 500 in the test set.

- We provide this dataset in the `dslabs` package:

```{r, warning=FALSE, message=FALSE} 
library(tidyverse) 
library(dslabs) 
data("mnist_27") 
``` 



## Case study: is it a 2 or a 7?

- We can explore the data by plotting the two predictors and using colors to denote the labels:

```{r two-or-seven-scatter, eval=FALSE} 
mnist_27$train |> ggplot(aes(x_1, x_2, color = y)) + geom_point() 
``` 


## Case study: is it a 2 or a 7?

```{r two-or-seven-scatter-run, echo=FALSE} 
mnist_27$train |> ggplot(aes(x_1, x_2, color = y)) + geom_point() 
``` 


## Case study: is it a 2 or a 7?

- We can immediately see some patterns.

- For example, if $X_1$ (the upper left panel) is very large, then the digit is probably a 7.

- Also, for smaller values of $X_1$, the 2s appear to be in the mid range values of $X_2$.

- To illustrate how to interpret $X_1$ and $X_2$, we include four example images.



## Case study: is it a 2 or a 7?

- On the left are the original images of the two digits with the largest and smallest values for $X_1$ and on the right we have the images corresponding to the largest and smallest values of $X_2$



## Case study: is it a 2 or a 7?

```{r two-or-seven-images-large-x1, echo=FALSE, out.width="100%", fig.height=3, fig.width=6.5} 
if(!exists("mnist")) mnist <- read_mnist() 
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_1), which.max(mnist_27$train$x_1))] 
titles <- c("smallest","largest") 
tmp <- lapply(1:2, function(i){ 
    expand.grid(Row=1:28, Column=1:28) |>   
      mutate(label=titles[i],   
             value = mnist$train$images[is[i],]) 
}) 
tmp <- Reduce(rbind, tmp) 
p1 <- tmp |> ggplot(aes(Row, Column, fill=value)) +  
  geom_raster(show.legend = FALSE) +  
  scale_y_reverse() + 
  scale_fill_gradient(low="white", high="black") + 
  facet_grid(.~label) +  
  geom_vline(xintercept = 14.5) + 
  geom_hline(yintercept = 14.5) + 
  ggtitle("Largest and smallest x_1") 
is <- mnist_27$index_train[c(which.min(mnist_27$train$x_2), which.max(mnist_27$train$x_2))] 
titles <- c("smallest","largest") 
tmp <- lapply(1:2, function(i){ 
    expand.grid(Row=1:28, Column=1:28) |>   
      mutate(label=titles[i],   
             value = mnist$train$images[is[i],]) 
}) 
tmp <- Reduce(rbind, tmp) 
p2 <- tmp |> ggplot(aes(Row, Column, fill=value)) +  
    geom_raster(show.legend = FALSE) +  
    scale_y_reverse() + 
    scale_fill_gradient(low="white", high="black") + 
    facet_grid(.~label) +  
    geom_vline(xintercept = 14.5) + 
    geom_hline(yintercept = 14.5) + 
  ggtitle("Largest and smallest x_2") 
gridExtra::grid.arrange(p1, p2, ncol = 2) 
``` 


## Case study: is it a 2 or a 7?

- We can start getting a sense for why these predictors are useful, but also why the problem will be somewhat challenging.

- We haven't really learned any algorithms yet, so let's try building an algorithm using regression.

- The model is simply:

$$
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

## Case study: is it a 2 or a 7?


- We fit it like this:

```{r} 
fit <- mnist_27$train |> 
  mutate(y = ifelse(y==7, 1, 0)) |> 
  lm(y ~ x_1 + x_2, data = _) 
``` 

## Case study: is it a 2 or a 7?

- We can now build a decision rule based on the estimate of $\hat{p}(x_1, x_2)$:

```{r, warning=FALSE} 
library(caret) 
p_hat <- predict(fit, newdata = mnist_27$test) 
y_hat <- factor(ifelse(p_hat > 0.5, 7, 2)) 
confusionMatrix(y_hat, mnist_27$test$y)$overall[["Accuracy"]] 
``` 



## Case study: is it a 2 or a 7?

- We get an accuracy well above 50%.

- Not bad for our first try.

- But can we do better?

- Because we constructed the `mnist_27` example and we had at our disposal 60,000 digits in just the MNIST dataset, we used this to build the _true_ conditional distribution $p(x_1, x_2)$.



## Case study: is it a 2 or a 7?

- Keep in mind that this is something we don't have access to in practice, but we include it in this example because it permits the comparison of $\hat{p}(x_1, x_2)$ to the true $p(x_1, x_2)$.

- This comparison teaches us the limitations of different algorithms.

- Let's do that here.

- We have stored the true $p(x_1,x_2)$ in the `mnist_27` object and can plot the image using the __ggplot2__ function `geom_raster()`.



## Case study: is it a 2 or a 7?

- We choose better colors and use the `stat_contour` function to draw a curve that separates pairs $(x_1,x_2)$ for which $p(x_1,x_2) > 0.5$ and pairs for which $p(x_1,x_2) < 0.5$:

```{r true-p-better-colors, eval=FALSE} 
mnist_27$true_p |> ggplot(aes(x_1, x_2, z = p, fill = p)) + 
  geom_raster() + 
  scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) + 
  stat_contour(breaks=c(0.5), color="black") 
``` 


## Case study: is it a 2 or a 7?

```{r true-p-better-colors-run, echo=FALSE} 
mnist_27$true_p |> ggplot(aes(x_1, x_2, z = p, fill = p)) + 
  geom_raster() + 
  scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) + 
  stat_contour(breaks=c(0.5), color="black") 
``` 


## Case study: is it a 2 or a 7?

- We are plotting the true $p(x_1, x_2)$.

- To start understanding the limitations of regression here, first note that with regression $\hat{p}(x_1,x_2)$ has to be a plane, and as a result the boundary defined by the decision rule is given by:

- $\hat{p}(x_1,x_2) = 0.5$, which implies the boundary can't be anything other than a straight line:

$$
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5 \implies \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 = 0.5  \\
\implies
 x_2 = (0.5-\hat{\beta}_0)/\hat{\beta}_2 - \hat{\beta}_1/\hat{\beta}_2 x_1
$$

- Note that for this boundary, $x_2$ is a linear function of $x_1$.

- This implies that our regression approach has no chance of capturing the non-linear nature of the true $p(x_1,x_2)$.



## Case study: is it a 2 or a 7?

- Below is a visual representation of $\hat{p}(x_1, x_2)$.

- We used the `squish` function from the **scales** package to constrain estimates to be between 0 and 1.

- We can see where the mistakes were made by also showing the data and the boundary.

- They mainly come from low values $x_1$ that have either high or low value of $x_2$.

- Regression can't catch this.



## Case study: is it a 2 or a 7?

```{r regression-p-hat, echo=FALSE, out.width="100%", fig.height=3, fig.width=7} 
p_hat <- predict(fit, newdata = mnist_27$true_p) 
p_hat <- scales::squish(p_hat, c(0, 1)) 
p1 <- mnist_27$true_p |> mutate(p_hat = p_hat) |> 
  ggplot(aes(x_1, x_2,  z=p_hat, fill=p_hat)) + 
  geom_raster() + 
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + 
  stat_contour(breaks=c(0.5), color="black")  
p2 <- mnist_27$true_p |> mutate(p_hat = p_hat) |> 
  ggplot() + 
  stat_contour(aes(x_1, x_2, z=p_hat), breaks=c(0.5), color="black") +  
  geom_point(mapping = aes(x_1, x_2, color=y), data = mnist_27$test)  
gridExtra::grid.arrange(p1, p2, ncol = 2) 
``` 


## Case study: is it a 2 or a 7?

- We need something more flexible: a method that permits estimates with shapes other than a plane.

- We are going to learn a few new algorithms based on different ideas and concepts.

- But what they all have in common is that they permit more flexible approaches.

- We will start by describing nearest neighbor and kernel approaches.



## Case study: is it a 2 or a 7?

- To introduce the concepts behinds these approaches, we will again start with a simple one-dimensional example and describe the concept of _smoothing_.

