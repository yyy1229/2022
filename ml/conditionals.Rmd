---
title: "Conditionals"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```

## Conditional probs and expectations

- In machine learning applications, we rarely can predict outcomes perfectly.

- For example, spam detectors often miss emails that are clearly spam, Siri often misunderstands the words we are saying, and your bank at times thinks your card was stolen when it was not.

- The most common reason for not being able to build perfect algorithms is that it is impossible.

- To see this, note that most datasets will include groups of observations with the same exact observed values for all predictors, but with different outcomes.



## Conditional probabilities and expectations

- Because our prediction rules are functions, equal inputs (the predictors) implies equal outputs (the predictions).

- Therefore, for a challenge in which the same predictors are associated with different outcomes across different individual observations, it is impossible to predict correctly for all these cases.

- We saw a simple example of this in the previous section: for any given height $x$, you will have both males and females that are $x$ inches tall.



## Conditional probs and expectations

- However, none of this means that we can't build useful algorithms that are much better than guessing, and in some cases better than expert opinions.

- To achieve this in an optimal way, we make use of probabilistic representations.

- Observations with the same observed values for the predictors may not all be the same, but we can assume that they all have the same probability of this class or that class.

- We will write this idea out mathematically for the case of categorical data.



## Conditional probs

- We use the notation $(X_1 = x_1,\dots,X_p=x_p)$ to represent the fact that we have observed values $x_1,\dots,x_p$ for covariates $X_1, \dots, X_p$.

- This does not imply that the outcome $Y$ will take a specific value.

- Instead, it implies a specific probability.

## Conditional probs

- In particular, we denote the _conditional probabilities_ for each class $k$:

$$
\mbox{Pr}(Y=k \mid X_1 = x_1,\dots,X_p=x_p), \, \mbox{for}\,k=1,\dots,K
$$

- To avoid writing out all the predictors, we will use the bold letters like this: $\mathbf{X} \equiv (X_1,\dots,X_p)$ and $\mathbf{x} \equiv (x_1,\dots,x_p)$.

## Conditional probs

- We will also use the following notation for the conditional probability of being class $k$:

$$
p_k(\mathbf{x}) = \mbox{Pr}(Y=k \mid \mathbf{X}=\mathbf{x}), \, \mbox{for}\, k=1,\dots,K
$$

- Note: We will be using the $p(x)$ notation to represent conditional probabilities as functions of the predictors.

- Do not confuse it with the $p$ that represents the number of predictors.



## Conditional probabilities

- These probabilities guide the construction of an algorithm that makes the best prediction: for any given $\mathbf{x}$, we will predict the class $k$ with the largest probability among $p_1(x), p_2(x), \dots p_K(x)$.

- In mathematical notation, we write it like this: 

$$\hat{Y} = \max_k p_k(\mathbf{x})$$

- In machine learning, we refer to this as _Bayes' Rule_.

- But keep in mind that this is a theoretical rule since in practice we don't know $p_k(\mathbf{x}), k=1,\dots,K$.



## Conditional probabilities

- In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning.

- The better our probability estimates $\hat{p}_k(\mathbf{x})$, the better our predictor:

$$\hat{Y} = \max_k \hat{p}_k(\mathbf{x})$$ 

- So what we will predict depends on two things: 1) how close are the $\max_k p_k(\mathbf{x})$ to 1 or 0 (perfect certainty).

- and 2) how close our estimates $\hat{p}_k(\mathbf{x})$ are to $p_k(\mathbf{x})$.



## Conditional probabilities

- We can't do anything about the first restriction as it is determined by the nature of the problem, so our energy goes into finding ways to best estimate conditional probabilities.

- The first restriction does imply that we have limits as to how well even the best possible algorithm can perform.

- You should get used to the idea that while in some challenges we will be able to achieve almost perfect accuracy, with digit readers for example, in others our success is restricted by the randomness of the process, with movie recommendations for example.



## Conditional probabilities

- Before we continue, it is important to remember that defining our prediction by maximizing the probability is not always optimal in practice and depends on the context.

- As discussed above, sensitivity and specificity may differ in importance.

- But even in these cases, having a good estimate of the $p_k(x), k=1,\dots,K$ will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish.

- For instance, we can simply change the cutoffs used to predict one outcome or the other.



## Conditional probabilities

- In the plane example, we may ground the plane anytime the probability of malfunction is higher than 1 in a million as opposed to the default 1/2 used when error types are equally undesired.



## Conditional expectations

- For binary data, you can think of the probability $\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})$ as the proportion of 1s in the stratum of the population for which $\mathbf{X}=\mathbf{x}$.

- Many of the algorithms we will learn can be applied to both categorical and continuous data due to the connection between _conditional probabilities_ and _conditional expectations_.



## Conditional expectations

- Because the expectation is the average of values $y_1,\dots,y_n$ in the population, in the case in which the $y$s are 0 or 1, the expectation is equivalent to the probability of randomly picking a one since the average is simply the proportion of ones:

$$
\mbox{E}(Y \mid \mathbf{X}=\mathbf{x})=\mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x})
$$



## Conditional expectations

- As a result, we often only use the expectation to denote both the conditional probability and conditional expectation.

- Just like with categorical outcomes, in most applications the same observed predictors do not guarantee the same continuous outcomes.

- Instead, we assume that the outcome follows the same conditional distribution.

- We will now explain why we use the conditional expectation to define our predictors.



## Conditional expectation minimizes squared loss function

- Why do we care about the conditional expectation in machine learning? This is because the expected value has an attractive mathematical property: it minimizes the MSE.

- Specifically, of all possible predictions $\hat{Y}$,

$$
\hat{Y} = \mbox{E}(Y \mid \mathbf{X}=\mathbf{x}) \, \mbox{ minimizes } \, \mbox{E}\{ (\hat{Y} - Y)^2  \mid  \mathbf{X}=\mathbf{x} \}.
$$



## Conditional expectation minimizes squared loss function

- Due to this property, a succinct description of the main task of machine learning is that we use data to estimate:

$$
f(\mathbf{x}) \equiv \mbox{E}( Y  \mid  \mathbf{X}=\mathbf{x} ).
$$

- for any set of features $\mathbf{x} = (x_1, \dots, x_p)$.

- Of course this is easier said than done, since this function can take any shape and $p$ can be very large.



## Conditional expectation minimizes squared loss function

- Consider a case in which we only have one predictor $x$.

- The expectation $\mbox{E}\{ Y  \mid  X=x \}$ can be any function of $x$: a line, a parabola, a sine wave, a step function, anything.

- It gets even more complicated when we consider instances with large $p$, in which case $f(\mathbf{x})$ is a function of a multidimensional vector $\mathbf{x}$.

- For example, in our digit reader example $p = 784$! **The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation.**.

