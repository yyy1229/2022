---
title: "Dimension Reduction"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```

##  Dimension reduction

```{r, echo=FALSE} 
rafalib::mypar() 
``` 

- A typical machine learning challenge will include a large number of predictors, which makes visualization somewhat challenging.

- We have shown methods for visualizing univariate and paired data, but plots that reveal relationships between many variables are more complicated in higher dimensions.

- For example, to compare each of the 784 features in our predicting digits example,  we would have to create, for example, 306,936 scatterplots.



##  Dimension reduction

- Creating one single scatter-plot of the data is impossible due to the high dimensionality.

- Here we describe powerful techniques useful for exploratory data analysis, among other things, generally referred to as _dimension reduction_.

- The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations.

- With fewer dimensions, visualization then becomes more feasible.



##  Dimension reduction

- The technique behind it all, the singular value decomposition, is also useful in other contexts.

- Principal component analysis (PCA) is the approach we will be showing.

- Before applying PCA to high-dimensional datasets, we will motivate the ideas behind with a simple example.



## Preserving distance

- We consider an example with twin heights.

- Some pairs are adults, the others are children.

- Here we simulate 100 two-dimensional points that represent the number of standard deviations each individual is from the mean height.

- Each point is a pair of twins.

- We use the `mvrnorm` function from the __MASS__ package to simulate bivariate normal data.



## Preserving distance

```{r, message=FALSE} 
set.seed(1988) 
library(MASS) 
n <- 100 
Sigma <- matrix(c(9, 9 * 0.9, 9 * 0.92, 9 * 1), 2, 2) 
x <- rbind(mvrnorm(n / 2, c(69, 69), Sigma), 
           mvrnorm(n / 2, c(55, 55), Sigma)) 
``` 

- A scatterplot quickly reveals that the correlation is high and that there are two groups of twins, the adults (upper right points) and the children (lower left points):



## Preserving distance

```{r simulated-twin-heights, fig.width=3, fig.asp=1, echo=FALSE, message=FALSE, out.width="50%"} 
lim <- c(48, 78) 
rafalib::mypar() 
plot(x, xlim=lim, ylim=lim) 
``` 


## Preserving distance

```{r distance-illustration, fig.width=3, fig.asp=1, echo=FALSE, out.width="50%"} 
rafalib::mypar() 
plot(x, xlim=lim, ylim=lim) 
lines(x[c(1, 2),], col = "blue", lwd = 2) 
lines(x[c(2, 51),], col = "red", lwd = 2) 
points(x[c(1, 2, 51),], pch = 16) 
``` 


## Preserving distance

- Our features are $N$ two-dimensional points, the two heights, and, for illustrative purposes, we will act as if visualizing two dimensions is too challenging.

- We therefore want to reduce the dimensions from two to one, but still be able to understand important characteristics of the data, for example that the observations cluster into two groups: adults and children.

- Let's consider a specific challenge: we want a one-dimensional summary of our predictors from which we can approximate the distance between any two observations.



## Preserving distance

- In the figure above we show the distance between observation 1 and 2 (blue), and observation 1 and 51 (red).

- Note that the blue line is shorter, which implies 1 and 2 are closer.

- We can compute these distances using `dist`:

```{r} 
d <- dist(x) 
as.matrix(d)[1, 2] 
as.matrix(d)[2, 51] 
``` 

- This distance is based on two dimensions and we need a distance approximation based on just one.



## Preserving distance

- Let's start with the naive approach of simply removing one of the two dimensions.

- Let's compare the actual distances to the distance computed with just the first dimension:

```{r} 
z <- x[,1] 
``` 




## Preserving distance

- Here are the approximate distances versus the original distances:


```{r one-dim-approx-to-dist, echo = FALSE, fig.width=3, fig.asp=1, out.width="50%"} 
rafalib::mypar() 
plot(dist(x), dist(z)) 
abline(0,1, col = "red") 
``` 


## Preserving distance

- The plot looks about the same if we use the second dimension.

- We obtain a general underestimation.

- This is to be expected because we are adding more positive quantities in the distance calculation as we increase the number of dimensions.

## Preserving distance

- If instead we use an average, like this.

$$\sqrt{ \frac{1}{2} \sum_{j=1}^2 (X_{1,j}-X_{2,j})^2 },$$ 

- then the underestimation goes away.


- We divide the distance by $\sqrt{2}$ to achieve the correction.



## Preserving distance

```{r distance-approx-1, echo = FALSE, fig.width=3, fig.asp=1, out.width="50%"} 
rafalib::mypar() 
plot(dist(x) / sqrt(2), dist(z)) 
abline(0, 1, col = "red") 
``` 


## Preserving distance

- This actually works pretty well and we get a typical difference of:

```{r} 
sd(dist(x) - dist(z)*sqrt(2)) 
``` 

- Now, can we pick a one-dimensional summary that makes this approximation even better?

- If we look back at the previous scatterplot and visualize a line between any pair of points, the length of this line is the distance between the two points.

- These lines tend to go along the direction of the diagonal.



## Preserving distance

- Notice that if we instead plot the difference versus the average:

```{r} 
z  <- cbind((x[,2] + x[,1])/2,  x[,2] - x[,1]) 
``` 

- we can see how the distance between points is mostly explained by the first dimension: the average.



## Preserving distance

```{r rotation, fig.width=3, fig.asp=1, echo=FALSE, out.width="50%"} 
rafalib::mypar() 
plot(z, xlim=lim, ylim = lim - mean(lim)) 
lines(z[c(1,2),], col = "blue", lwd = 2) 
lines(z[c(2,51),], col = "red", lwd = 2) 
points(z[c(1,2,51),], pch = 16) 
``` 


## Preserving distance

- This means that we can ignore the second dimension and not lose too much information.

- If the line is completely flat, we lose no information at all.




## Preserving distance

- Using the first dimension of this transformed matrix we obtain an even better approximation:

```{r distance-approx-2, echo=FALSE, fig.width=3, fig.asp=1, out.width="50%"} 
rafalib::mypar() 
plot(dist(x), dist(z[,1])*sqrt(2)) 
abline(0,1, col = "red") 
``` 


## Preserving distance

- with the typical difference improved by about 35%:

```{r} 
sd(dist(x) - dist(z[,1])*sqrt(2)) 
``` 

- Later we learn that `z[,1]` is the first principal component of the matrix `x`.



## Linear transformations

- Note that each row of $X$ was transformed using a linear transformation.

- For any row $i$, the first entry was:

$$Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}$$ 

- with $a_{1,1} = 0.5$ and $a_{2,1} = 0.5$.

- The second entry was also a linear transformation:

$$Z_{i,2} = a_{1,2} X_{i,1} + a_{2,2} X_{i,2}$$ 

- with $a_{1,2} = 1$ and $a_{2,2} = -1$.



## Linear transformations

- We can also use linear transformation to get $X$ back from $Z$:

$$X_{i,1} = b_{1,1} Z_{i,1} + b_{2,1} Z_{i,2}$$ 

- with $b_{1,2} = 1$ and $b_{2,1} = 0.5$ and.

$$X_{i,2} = b_{2,1} Z_{i,1} + b_{2,2} Z_{i,2}$$ 

- with $b_{2,1} = 1$ and $a_{1,2} = -0.5$.




## Linear transformations

- We can write the operation we just performed like this:


$$
Z = X A 
\mbox{ with } 
A = \, 
\begin{pmatrix} 
1/2&1\\ 
1/2&-1\\ 
\end{pmatrix}. 
$$



## Linear transformations

- And that we can transform back by simply multiplying by $A^{-1}$ as follows:


$$
X = Z A^{-1}  
\mbox{ with } 
A^{-1} = \, 
\begin{pmatrix} 
1&1\\ 
1/2&-1/2\\ 
\end{pmatrix}. 
$$



## Linear transformations

- Dimension reduction can often be described as applying a transformation $A$ to a matrix $X$ with many columns that _moves_ the information contained in $X$ to the first few columns of $Z=AX$, then keeping just these few informative columns, thus reducing the dimension of the vectors contained in the rows.



## Orthogonal transformations

- Note that we divided the above by $\sqrt{2}$ to account for the differences in dimensions when comparing a 2 dimension distance to a 1 dimension distance.

- We can actually guarantee that the distance scales remain the same if we re-scale the columns of $A$ to assure that the sum of squares is 1.

$$a_{1,1}^2 + a_{2,1}^2 = 1\mbox{ and } a_{1,2}^2 + a_{2,2}^2=1,$$ 

- and that the correlation of the columns is 0:


$$
a_{1,1} a_{1,2} + a_{2,1} a_{2,2} = 0. 
$$



## Orthogonal transformations

- Remember that if the columns are centered to have average 0, then the sum of squares is equivalent to the variance or standard deviation squared.

- In our example, to achieve orthogonality, we multiply the first set of coefficients (first column of $A$) by $\sqrt{2}$ and the second by $1/\sqrt{2}$, then we get the same exact distance if we use both dimensions:

```{r} 
z[,1] <- (x[,1] + x[,2]) / sqrt(2) 
z[,2] <- (x[,2] - x[,1]) / sqrt(2) 
``` 




## Orthogonal transformations

- This gives us a transformation that preserves the distance between any two points:

```{r} 
max(dist(z) - dist(x)) 
``` 

```{r orthogonal-transformation, fig.width=3, fig.asp=1, echo=FALSE, out.width="40%"} 
rafalib::mypar() 
plot(dist(x), dist(z)) 
abline(0, 1, col = "red") 
``` 


## Orthogonal transformations


- and an improved approximation if we use just the first dimension:

```{r} 
sd(dist(x) - dist(z[,1])) 
``` 

- In this case $Z$ is called an orthogonal rotation of $X$: it preserves the distances between rows.

- Note that by using the transformation above we can summarize the distance between any two pairs of twins with just one dimension.



## Orthogonal transformations

- For example, one-dimensional data exploration of the first dimension of $Z$ clearly shows that there are two groups, adults and children:

```{r twins-pc-1-hist, message=FALSE, warning=FALSE, eval=FALSE} 
library(tidyverse) 
qplot(z[,1], bins = 20, color = I("black")) 
``` 


## Orthogonal transformations

```{r twins-pc-1-hist-run, message=FALSE, warning=FALSE, echo=FALSE} 
library(tidyverse) 
qplot(z[,1], bins = 20, color = I("black")) 
``` 


## Orthogonal transformations

- We successfully reduced the number of dimensions from two to one with very little loss of information.

- The reason we were able to do this is because the columns of $X$ were very correlated:

```{r} 
cor(x[,1], x[,2]) 
``` 

- and the transformation produced uncorrelated columns with "independent" information in each column:

```{r} 
cor(z[,1], z[,2]) 
``` 




## Orthogonal transformations

- One way this insight may be useful in a machine learning application is that we can reduce the complexity of a model by using just $Z_1$ rather than both $X_1$ and $X_2$.

- It is actually common to obtain data with several highly correlated predictors.

- In these cases PCA, which we describe next, can be quite useful for reducing the complexity of the model being fit.



## Principal component analysis

- In the computation above, the total variability in our data can be defined as the sum of the sum of squares of the columns.

- We assume the columns are centered, so this sum is equivalent to the sum of the variances of each column:


$$
v_1 + v_2, \mbox{ with } v_1 = \frac{1}{N}\sum_{i=1}^N X_{i,1}^2 \mbox{ and } v_2 =  \frac{1}{N}\sum_{i=1}^N X_{i,2}^2  
$$

- We can compute $v_1$ and $v_2$ using:

```{r} 
colMeans(x^2)  
``` 



## Principal component analysis

- and we can show mathematically that if we apply an orthogonal transformation as above, then the total variation remains the same:

```{r} 
sum(colMeans(x^2)) 
sum(colMeans(z^2)) 
``` 

- However, while the variability in the two columns of `X` is about the same, in the transformed version $Z$ more than 99% of the variability is included in just the first dimension:

```{r, echo=FALSE}
options(digits = 6)
```

```{r} 
v <- colMeans(z^2) 
v/sum(v) 
``` 

```{r, echo=FALSE}
options(digits = 3)
```


## Principal component analysis

- The _first principal component (PC)_ of a matrix $X$ is the linear orthogonal transformation of $X$ that maximizes this variability.

- The function `prcomp` provides this info:

```{r} 
pca <- prcomp(x) 
pca$rotation 
``` 

- Note that the first PC is almost the same as that provided by the  $(X_1 + X_2) / \sqrt{2}$ we used earlier (except perhaps for a sign change that is arbitrary).



## Principal component analysis

- The function PCA returns both the rotation needed to transform $X$ so that the variability of the columns is decreasing from most variable to least (accessed with `$rotation`) as well as the resulting new matrix (accessed with `$x`).

- By default the columns of $X$ are first centered.

- So, using the matrix multiplication shown above, we have that the following are the same (demonstrated by a difference between elements of essentially zero):

```{r} 
a <- sweep(x, 2, colMeans(x))  
b <- pca$x %*% t(pca$rotation) 
max(abs(a - b)) 
``` 



## Principal component analysis

- The rotation is orthogonal which means that the inverse is its transpose.

- So we also have that these two are identical:

```{r} 
a <- sweep(x, 2, colMeans(x)) %*% pca$rotation 
b <- pca$x  
max(abs(a - b)) 
``` 

- We can visualize these to see how the first component summarizes the data.




## Principal component analysis

- In the plot below red represents high values and blue negative values:

```{r illustrate-pca-twin-heights, echo=FALSE, height = 5, out.width="60%"} 
illustrate_pca <- function(x, flip=1,  
                           pad = round((nrow(x)/2-ncol(x))*1/4),  
                           cex = 5, center = TRUE){ 
  rafalib::mypar(1,5) 
  ## flip is because PCA chooses arbitrary sign for loadings and PC 
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu")) 
  pca <- prcomp(x, center = center) 
  if(center) z <- t(x) - rowMeans(t(x)) 
  cols <- 1:ncol(x) 
  rows <- 1:nrow(x) 
  image(cols, rows, z[,rev(1:ncol(z))], xaxt = "n", yaxt = "n",  
        xlab="", ylab="", main= "X", col = colors) 
  abline(h=rows + 0.5, v = cols + 0.5) 
  rafalib::nullplot(xaxt="n",yaxt="n",bty="n") 
  text(0.5, 0.5, "=", cex = cex) 
  z <- flip*t(pca$x) 
  image(cols, rows, z[,rev(1:ncol(z))], xaxt = "n", yaxt = "n",xlab="",ylab="", main= "Weights", col = colors) 
  abline(h=rows + 0.5, v = cols + 0.5) 
  rafalib::nullplot(xaxt="n",yaxt="n",bty="n") 
  text(0.5, 0.5, "x", cex = cex) 
  z <- flip*pca$rotation 
  nz <- cbind(matrix(NA, ncol(z), pad), z, matrix(NA, ncol(z), pad)) 
  rows <- 1:ncol(nz) 
  image(cols, rows, nz[,rev(1:ncol(nz))],  xaxt = "n", yaxt = "n", bty = "n", xlab="",ylab="", col = colors) 
  abline(h = pad+0:ncol(z)+1/2) 
  lines(c(ncol(z)/2+0.5,ncol(z)/2+1/2),c(pad,pad+ncol(z))+0.5) 
  text(ncol(z)/2+0.5, pad+ncol(z)+2 , expression(bold(Pattern^T)), font=2) 
} 
rafalib::mypar(1,1) 
illustrate_pca(x, flip = -1) 
``` 


## Principal component analysis

- It turns out that we can find this linear transformation not just for two dimensions but for matrices of any dimension $p$.

- For a multidimensional matrix with $X$ with $p$ columns, we can find a transformation that creates $Z$ that preserves distance between rows, but with the variance of the columns in decreasing order.

- The second column is the second principal component, the third column is the third principal component, and so on.



## Principal component analysis

- As in our example, if after a certain number of columns, say $k$, the variances of the columns of $Z_j$, $j>k$ are very small, it means these dimensions have little to contribute to the distance and we can approximate distance between any two points with just $k$ dimensions.

- If $k$ is much smaller than $p$, then we can achieve a very efficient summary of our data.



## Iris example

- The iris data is a widely used example in data analysis courses.

- It includes four botanical measurements related to three flower species:

```{r} 
names(iris) 
``` 

- If you print `iris$Species` you will see that the data is ordered by the species.

- Let's compute the distance between each observation.




## Iris example

- You can clearly see the three species with one species very different from the other two:

```{r, eval=FALSE} 
x <- iris[,1:4] |> as.matrix() 
d <- dist(x) 
image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu"))) 
``` 

```{r iris-distances, fig.width = 4, fig.asp=1, out.width="40%", echo=FALSE} 
rafalib::mypar() 
x <- iris[,1:4] |> as.matrix() 
d <- dist(x) 
image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu"))) 
``` 

## Iris example

- Our predictors here have four dimensions, but three are very correlated:

```{r} 
cor(x) 
``` 

- If we apply PCA, we should be able to approximate this distance with just two dimensions, compressing the highly correlated dimensions.



## Iris example

- Using the `summary` function we can see the variability explained by each PC:

```{r} 
pca <- prcomp(x) 
summary(pca) 
``` 

## Iris example

- The first two dimensions account for 97% of the variability.

- Thus we should be able to approximate the distance very well with two dimensions.

## Iris example

- We can visualize the results of PCA:

```{r illustrate-pca-twin-heights-iris, echo=FALSE, fig.height = 6, out.width="60%"} 
rafalib::mypar() 
illustrate_pca(x) 
``` 



## Iris example

- And see that the first pattern is  sepal length,  petal length, and petal width (red) in one direction and sepal width (blue) in the other.

- The second pattern is the sepal length and petal width in one direction (blue) and petal length and petal width in the other (red).

- You can see from the weights that the first PC1 drives most of the variability and it clearly separates the first third of samples (setosa) from the second two thirds (versicolor and virginica).



## Iris example

- If you look at the second column of the weights, you notice that it somewhat separates versicolor (red) from virginica (blue).

- We can see this better by plotting the first two PCs with color representing the species:

```{r iris-pca, eval=FALSE} 
data.frame(pca$x[,1:2], Species=iris$Species) |>  
  ggplot(aes(PC1,PC2, fill = Species))+ 
  geom_point(cex=3, pch=21) + 
  coord_fixed(ratio = 1) 
``` 


## Iris example

```{r iris-pca-run, echo=FALSE} 
data.frame(pca$x[,1:2], Species=iris$Species) |>  
  ggplot(aes(PC1,PC2, fill = Species))+ 
  geom_point(cex=3, pch=21) + 
  coord_fixed(ratio = 1) 
``` 


## Iris example

- We see that the first two dimensions preserve the distance:

```{r dist-approx-4, message = FALSE, fig.height = 3, fig.asp = 1, out.width="60%", eval=FALSE} 
d_approx <- dist(pca$x[, 1:2]) 
qplot(d, d_approx) + geom_abline(color="red") 
``` 


## Iris example

```{r dist-approx-4-run, message = FALSE, fig.height = 3, fig.asp = 1, out.width="50%", echo=FALSE} 
d_approx <- dist(pca$x[, 1:2]) 
qplot(d, d_approx) + geom_abline(color="red") 
``` 


## Iris example

- This example is more realistic than the first artificial example we used, since we showed how we can visualize the data using two dimensions when the data was four-dimensional.



## MNIST example

- The written digits example has 784 features.

- Is there any room for data reduction? Can we create simple machine learning algorithms using fewer features?

- Let's load the data:

```{r} 
library(dslabs) 
if(!exists("mnist")) mnist <- read_mnist() 
``` 

- Because the pixels are so small, we expect pixels close to each other on the grid to be correlated, meaning that dimension reduction should be possible.

- Let's try PCA and explore the variance of the PCs.



## MNIST example

- This will take a few seconds as it is a rather large matrix.

```{r, cache=TRUE} 
col_means <- colMeans(mnist$test$images) 
pca <- prcomp(mnist$train$images) 
``` 

```{r mnist-pca-variance-explained, eval=FALSE} 
pc <- 1:ncol(mnist$test$images) 
qplot(pc, pca$sdev) 
``` 


## MNIST example

```{r mnist-pca-variance-explained-run, echo=FALSE} 
pc <- 1:ncol(mnist$test$images) 
qplot(pc, pca$sdev) 
``` 


## MNIST example

- We can see that the first few PCs already explain a large percent of the variability:

```{r} 
summary(pca)$importance[,1:5]  
``` 

## MNIST example

- And just by looking at the first two PCs we see information about the class.

- Here is a random sample of 2,000 digits:

```{r mnist-pca-1-2-scatter, eval=FALSE} 
data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], 
           label=factor(mnist$train$label)) |> 
  sample_n(2000) |>  
  ggplot(aes(PC1, PC2, fill=label))+ 
  geom_point(cex=3, pch=21) 
``` 


## MNIST example

```{r mnist-pca-1-2-scatter-run, echo=FALSE} 
data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], 
           label=factor(mnist$train$label)) |> 
  sample_n(2000) |>  
  ggplot(aes(PC1, PC2, fill=label))+ 
  geom_point(cex=3, pch=21) 
``` 


## MNIST example

- We can also _see_ the linear combinations on the grid to get an idea of what is getting weighted:


```{r mnist-pca-1-4, echo = FALSE, out.width="100%", fig.width=6, fig.asp=0.3} 
library(RColorBrewer) 
tmp <- lapply( c(1:4,781:784), function(i){ 
    expand.grid(Row=1:28, Column=1:28) |> 
      mutate(id=i, label=paste0("PC",i),  
             value = pca$rotation[,i]) 
}) 
tmp <- Reduce(rbind, tmp) 
tmp |> filter(id<5) |> 
  ggplot(aes(Row, Column, fill=value)) + 
  geom_raster() + 
  scale_y_reverse() + 
  scale_fill_gradientn(colors = brewer.pal(9, "RdBu")) + 
  facet_wrap(~label, nrow = 1) 
``` 


## MNIST example

- The lower variance PCs appear related to unimportant variability in the corners:


```{r mnist-pca-last,, echo = FALSE, out.width="100%", fig.width=6, fig.asp=0.3} 
tmp |> filter(id>5) |> 
  ggplot(aes(Row, Column, fill=value)) + 
  geom_raster() + 
  scale_y_reverse() + 
  scale_fill_gradientn(colors = brewer.pal(9, "RdBu")) + 
  facet_wrap(~label, nrow = 1) 
``` 


## MNIST example

- Now let's apply the transformation we learned with the training data to the test data, reduce the dimension and run knn on just a small number of dimensions.

- We try 36 dimensions since this explains about 80% of the data.

## MNIST example

- First fit the model:

```{r} 
library(caret) 
k <- 36 
x_train <- pca$x[,1:k] 
y <- factor(mnist$train$labels) 
fit <- knn3(x_train, y) 
``` 



## MNIST example

- Now transform the test set:

```{r} 
x_test <- sweep(mnist$test$images, 2, col_means) %*% pca$rotation 
x_test <- x_test[,1:k] 
``` 

## MNIST example

- And we are ready to predict and see how we do:

```{r} 
y_hat <- predict(fit, x_test, type = "class") 
confusionMatrix(y_hat, factor(mnist$test$labels))$overall["Accuracy"] 
``` 

- With just 36 dimensions  we get an accuracy well above 0.95.

